{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Market Prediction - Enhanced v1 (1105)\n",
    "\n",
    "**Date**: 2025-11-05  \n",
    "**Improvements from 1104_v1**:\n",
    "- ðŸš€ Advanced feature engineering (100+ features)\n",
    "- ðŸ¤– XGBoost + LightGBM ensemble\n",
    "- ðŸ“Š Continuous allocation strategy with volatility scaling\n",
    "- ðŸ“¤ Kaggle submission API implementation\n",
    "\n",
    "**Target**: Kaggle-ready submission with improved performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('ðŸš€ Libraries loaded!')\n",
    "print(f'LightGBM version: {lgb.__version__}')\n",
    "print(f'XGBoost version: {xgb.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f'Train shape: {train.shape}')\n",
    "print(f'Test shape: {test.shape}')\n",
    "\n",
    "# Use only complete data (date_id >= 5540 from EDA)\n",
    "feature_cols = [c for c in train.columns if c not in ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']]\n",
    "train['n_missing'] = train[feature_cols].isnull().sum(axis=1)\n",
    "\n",
    "# Keep data with <5% missing\n",
    "threshold_missing = len(feature_cols) * 0.05\n",
    "df = train[train['n_missing'] < threshold_missing].copy()\n",
    "\n",
    "print(f'\\nUsing complete data: {len(df)} days')\n",
    "print(f'Date range: {df[\"date_id\"].min()} to {df[\"date_id\"].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Advanced Feature Engineering\n",
    "\n",
    "### Strategy:\n",
    "1. **Lag features**: 1, 5, 10, 20, 60 days\n",
    "2. **Rolling statistics**: mean, std, min, max (5, 10, 20, 60 windows)\n",
    "3. **Momentum indicators**: 5, 20, 60 days\n",
    "4. **Volatility features**: Historical volatility, volatility of volatility\n",
    "5. **Market regime features**: Bull/bear, high/low volatility\n",
    "6. **Cross-sectional features**: Category means, dispersions\n",
    "7. **Feature interactions**: Top feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_features(data, feature_cols):\n",
    "    \"\"\"\n",
    "    Create advanced feature engineering\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    print('Starting feature engineering...')\n",
    "    \n",
    "    # Select top correlated features for lag/rolling (reduce computation)\n",
    "    if 'forward_returns' in df.columns:\n",
    "        correlations = df[feature_cols + ['forward_returns']].corr()['forward_returns'].drop('forward_returns')\n",
    "        top_features = correlations.abs().nlargest(15).index.tolist()\n",
    "    else:\n",
    "        # For test set, use predefined top features\n",
    "        top_features = ['P8', 'P10', 'S5', 'E3', 'M3', 'V13', 'P11', 'E2', 'P12', 'M1', 'S7', 'E5', 'M9', 'V7', 'I5']\n",
    "    \n",
    "    print(f'Using {len(top_features)} top features for intensive engineering')\n",
    "    \n",
    "    # 1. Lag features (1, 5, 10, 20, 60)\n",
    "    print('Creating lag features...')\n",
    "    for feat in top_features:\n",
    "        for lag in [1, 5, 10, 20, 60]:\n",
    "            df[f'{feat}_lag{lag}'] = df[feat].shift(lag)\n",
    "    \n",
    "    # 2. Rolling statistics\n",
    "    print('Creating rolling statistics...')\n",
    "    for feat in top_features:\n",
    "        for window in [5, 20, 60]:\n",
    "            df[f'{feat}_mean{window}'] = df[feat].rolling(window).mean()\n",
    "            df[f'{feat}_std{window}'] = df[feat].rolling(window).std()\n",
    "    \n",
    "    # 3. Returns-based features\n",
    "    if 'forward_returns' in df.columns:\n",
    "        print('Creating returns-based features...')\n",
    "        # Historical returns (shifted to avoid leakage)\n",
    "        df['returns_lag1'] = df['forward_returns'].shift(1)\n",
    "        df['returns_lag5'] = df['forward_returns'].shift(5)\n",
    "        \n",
    "        # Momentum\n",
    "        df['momentum_5'] = df['forward_returns'].shift(1).rolling(5).sum()\n",
    "        df['momentum_20'] = df['forward_returns'].shift(1).rolling(20).sum()\n",
    "        df['momentum_60'] = df['forward_returns'].shift(1).rolling(60).sum()\n",
    "        \n",
    "        # Rolling returns statistics\n",
    "        df['returns_mean_20'] = df['forward_returns'].shift(1).rolling(20).mean()\n",
    "        df['returns_std_20'] = df['forward_returns'].shift(1).rolling(20).std()\n",
    "        df['returns_mean_60'] = df['forward_returns'].shift(1).rolling(60).mean()\n",
    "        df['returns_std_60'] = df['forward_returns'].shift(1).rolling(60).std()\n",
    "        \n",
    "        # Volatility\n",
    "        df['volatility_5'] = df['forward_returns'].shift(1).rolling(5).std()\n",
    "        df['volatility_20'] = df['forward_returns'].shift(1).rolling(20).std()\n",
    "        df['volatility_60'] = df['forward_returns'].shift(1).rolling(60).std()\n",
    "        \n",
    "        # Volatility of volatility\n",
    "        df['vol_of_vol_20'] = df['volatility_20'].rolling(20).std()\n",
    "        \n",
    "        # Volatility regime\n",
    "        vol_percentile = df['volatility_20'].rolling(252).rank(pct=True)\n",
    "        df['vol_regime_low'] = (vol_percentile < 0.33).astype(int)\n",
    "        df['vol_regime_high'] = (vol_percentile > 0.67).astype(int)\n",
    "        \n",
    "        # Returns regime\n",
    "        df['bull_regime'] = (df['returns_mean_60'] > 0).astype(int)\n",
    "        \n",
    "        # Risk-adjusted momentum\n",
    "        df['risk_adj_momentum_20'] = df['momentum_20'] / (df['volatility_20'] + 1e-8)\n",
    "        df['risk_adj_momentum_60'] = df['momentum_60'] / (df['volatility_60'] + 1e-8)\n",
    "    \n",
    "    # 4. Cross-sectional features (category aggregations)\n",
    "    print('Creating cross-sectional features...')\n",
    "    feature_categories = {\n",
    "        'M': [c for c in feature_cols if c.startswith('M')],\n",
    "        'E': [c for c in feature_cols if c.startswith('E')],\n",
    "        'I': [c for c in feature_cols if c.startswith('I')],\n",
    "        'P': [c for c in feature_cols if c.startswith('P')],\n",
    "        'V': [c for c in feature_cols if c.startswith('V')],\n",
    "        'S': [c for c in feature_cols if c.startswith('S')],\n",
    "    }\n",
    "    \n",
    "    for cat, feats in feature_categories.items():\n",
    "        if feats:\n",
    "            df[f'{cat}_mean'] = df[feats].mean(axis=1)\n",
    "            df[f'{cat}_std'] = df[feats].std(axis=1)\n",
    "            df[f'{cat}_min'] = df[feats].min(axis=1)\n",
    "            df[f'{cat}_max'] = df[feats].max(axis=1)\n",
    "    \n",
    "    # 5. Feature interactions (top pairs)\n",
    "    print('Creating feature interactions...')\n",
    "    if len(top_features) >= 2:\n",
    "        # Create interactions for top 5 features\n",
    "        for i in range(min(5, len(top_features))):\n",
    "            for j in range(i+1, min(5, len(top_features))):\n",
    "                feat1, feat2 = top_features[i], top_features[j]\n",
    "                df[f'{feat1}_x_{feat2}'] = df[feat1] * df[feat2]\n",
    "    \n",
    "    # 6. Missing value indicator\n",
    "    df['n_missing_orig'] = df['n_missing']\n",
    "    \n",
    "    print(f'Feature engineering complete! Shape: {df.shape}')\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = create_advanced_features(df, feature_cols)\n",
    "\n",
    "# Drop rows with NaN from rolling/lag features\n",
    "df_clean = df_engineered.dropna()\n",
    "print(f'\\nFinal data shape after dropping NaN: {df_clean.shape}')\n",
    "\n",
    "# Identify feature list (exclude targets and meta)\n",
    "exclude_cols = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'n_missing']\n",
    "feature_list = [c for c in df_clean.columns if c not in exclude_cols]\n",
    "print(f'Total features: {len(feature_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Competition Metric & Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_competition_score(returns, risk_free_rate, position):\n",
    "    \"\"\"\n",
    "    Calculate competition metric (volatility-adjusted Sharpe ratio)\n",
    "    \"\"\"\n",
    "    MIN_INVESTMENT = 0\n",
    "    MAX_INVESTMENT = 2\n",
    "    \n",
    "    if isinstance(position, (int, float)):\n",
    "        position = pd.Series([position] * len(returns), index=returns.index)\n",
    "    \n",
    "    # Validate\n",
    "    if position.max() > MAX_INVESTMENT or position.min() < MIN_INVESTMENT:\n",
    "        return {'error': 'Position out of range'}\n",
    "    \n",
    "    # Strategy returns\n",
    "    strategy_returns = risk_free_rate * (1 - position) + position * returns\n",
    "    \n",
    "    # Strategy metrics\n",
    "    strategy_excess_returns = strategy_returns - risk_free_rate\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(returns)) - 1\n",
    "    strategy_std = strategy_returns.std()\n",
    "    \n",
    "    trading_days_per_yr = 252\n",
    "    if strategy_std == 0:\n",
    "        return {'error': 'Strategy std is zero'}\n",
    "    \n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "    \n",
    "    # Market metrics\n",
    "    market_excess_returns = returns - risk_free_rate\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(returns)) - 1\n",
    "    market_std = returns.std()\n",
    "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "    \n",
    "    if market_volatility == 0:\n",
    "        return {'error': 'Market std is zero'}\n",
    "    \n",
    "    # Penalties\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2)\n",
    "    vol_penalty = 1 + excess_vol\n",
    "    \n",
    "    return_gap = max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "    \n",
    "    # Adjusted Sharpe\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    \n",
    "    return {\n",
    "        'score': min(float(adjusted_sharpe), 1_000_000),\n",
    "        'sharpe': sharpe,\n",
    "        'strategy_volatility': strategy_volatility,\n",
    "        'market_volatility': market_volatility,\n",
    "        'vol_ratio': strategy_volatility / market_volatility,\n",
    "        'vol_penalty': vol_penalty,\n",
    "        'return_penalty': return_penalty,\n",
    "    }\n",
    "\n",
    "def walk_forward_split(df, n_splits=3):\n",
    "    \"\"\"\n",
    "    Time-based walk-forward split\n",
    "    \"\"\"\n",
    "    total_len = len(df)\n",
    "    val_size = total_len // (n_splits + 1)\n",
    "    \n",
    "    splits = []\n",
    "    for i in range(n_splits):\n",
    "        train_end = val_size * (i + 2)\n",
    "        val_start = train_end\n",
    "        val_end = train_end + val_size\n",
    "        \n",
    "        if val_end > total_len:\n",
    "            break\n",
    "        \n",
    "        train_idx = df.index[:train_end]\n",
    "        val_idx = df.index[val_start:val_end]\n",
    "        \n",
    "        if len(val_idx) > 0:\n",
    "            splits.append((train_idx, val_idx))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Create splits\n",
    "splits = walk_forward_split(df_clean, n_splits=3)\n",
    "print(f'Created {len(splits)} walk-forward splits:')\n",
    "for i, (train_idx, val_idx) in enumerate(splits):\n",
    "    print(f'  Split {i+1}: Train={len(train_idx)}, Val={len(val_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Model Training - LightGBM & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_model(train_data, val_data, features, params=None):\n",
    "    \"\"\"\n",
    "    Train LightGBM model\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.7,\n",
    "            'max_depth': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': 42,\n",
    "        }\n",
    "    \n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['forward_returns']\n",
    "    X_val = val_data[features]\n",
    "    y_val = val_data['forward_returns']\n",
    "    \n",
    "    train_set = lgb.Dataset(X_train, y_train)\n",
    "    val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=200,\n",
    "        valid_sets=[val_set],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_xgb_model(train_data, val_data, features, params=None):\n",
    "    \"\"\"\n",
    "    Train XGBoost model\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'seed': 42,\n",
    "        }\n",
    "    \n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['forward_returns']\n",
    "    X_val = val_data[features]\n",
    "    y_val = val_data['forward_returns']\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    evals = [(dval, 'validation')]\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=200,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=30,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print('Model training functions ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Continuous Allocation Strategy\n",
    "\n",
    "### Approach:\n",
    "- Use model prediction as base signal\n",
    "- Apply sigmoid transformation for continuous allocation (0-2 range)\n",
    "- Dynamic volatility scaling to respect 120% constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_allocation(predicted_returns, recent_volatility=None, market_vol=None, \n",
    "                       base_allocation=1.0, sensitivity=100):\n",
    "    \"\"\"\n",
    "    Convert predicted returns to continuous allocation (0-2)\n",
    "    \n",
    "    Args:\n",
    "        predicted_returns: Model predictions\n",
    "        recent_volatility: Recent strategy volatility (optional)\n",
    "        market_vol: Market volatility (optional)\n",
    "        base_allocation: Center point (default 1.0)\n",
    "        sensitivity: How sensitive allocation is to predictions (higher = more extreme)\n",
    "    \n",
    "    Returns:\n",
    "        allocation: Continuous values in [0, 2]\n",
    "    \"\"\"\n",
    "    # Sigmoid transformation: maps predictions to (0, 2)\n",
    "    # predicted_returns near 0 -> allocation near 1.0\n",
    "    # positive predictions -> higher allocation\n",
    "    # negative predictions -> lower allocation\n",
    "    \n",
    "    allocation = base_allocation + np.tanh(predicted_returns * sensitivity)\n",
    "    \n",
    "    # Volatility-based scaling\n",
    "    if recent_volatility is not None and market_vol is not None and market_vol > 0:\n",
    "        vol_ratio = recent_volatility / market_vol\n",
    "        \n",
    "        # If approaching 120% threshold, scale down\n",
    "        if vol_ratio > 1.1:  # Start scaling at 110%\n",
    "            scaling_factor = 1.1 / vol_ratio\n",
    "            allocation = allocation * scaling_factor\n",
    "    \n",
    "    # Clip to valid range\n",
    "    allocation = np.clip(allocation, 0, 2)\n",
    "    \n",
    "    return allocation\n",
    "\n",
    "print('Allocation strategy function ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Train Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters from hyperparameter search (can be tuned further)\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.7,\n",
    "    'max_depth': 5,\n",
    "    'verbose': -1,\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# Cross-validation with ensemble\n",
    "results = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'FOLD {fold_idx + 1}/{len(splits)}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    train_data = df_clean.loc[train_idx]\n",
    "    val_data = df_clean.loc[val_idx].copy()\n",
    "    \n",
    "    # Train LightGBM\n",
    "    print('Training LightGBM...')\n",
    "    lgb_model = train_lgb_model(train_data, val_data, feature_list, lgb_params)\n",
    "    val_data['pred_lgb'] = lgb_model.predict(val_data[feature_list])\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print('Training XGBoost...')\n",
    "    xgb_model = train_xgb_model(train_data, val_data, feature_list, xgb_params)\n",
    "    val_data['pred_xgb'] = xgb_model.predict(xgb.DMatrix(val_data[feature_list]))\n",
    "    \n",
    "    # Ensemble prediction (weighted average)\n",
    "    val_data['pred_ensemble'] = 0.5 * val_data['pred_lgb'] + 0.5 * val_data['pred_xgb']\n",
    "    \n",
    "    # Calculate allocation\n",
    "    val_data['allocation'] = predict_allocation(\n",
    "        val_data['pred_ensemble'],\n",
    "        sensitivity=100,\n",
    "        base_allocation=1.0\n",
    "    )\n",
    "    \n",
    "    # Calculate score\n",
    "    score_result = calculate_competition_score(\n",
    "        val_data['forward_returns'],\n",
    "        val_data['risk_free_rate'],\n",
    "        val_data['allocation']\n",
    "    )\n",
    "    \n",
    "    if 'error' not in score_result:\n",
    "        print(f'\\nFold {fold_idx + 1} Results:')\n",
    "        print(f'  Score: {score_result[\"score\"]:.4f}')\n",
    "        print(f'  Sharpe: {score_result[\"sharpe\"]:.4f}')\n",
    "        print(f'  Vol Ratio: {score_result[\"vol_ratio\"]:.4f}')\n",
    "        print(f'  Vol Penalty: {score_result[\"vol_penalty\"]:.4f}')\n",
    "        print(f'  Return Penalty: {score_result[\"return_penalty\"]:.4f}')\n",
    "        \n",
    "        results.append({\n",
    "            'fold': fold_idx + 1,\n",
    "            'score': score_result['score'],\n",
    "            'sharpe': score_result['sharpe'],\n",
    "            'vol_ratio': score_result['vol_ratio'],\n",
    "        })\n",
    "    else:\n",
    "        print(f'Error in fold {fold_idx + 1}: {score_result[\"error\"]}')\n",
    "\n",
    "# Summary\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print('CROSS-VALIDATION SUMMARY')\n",
    "print(f'{\"=\"*80}')\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "print(f'\\nMean Score: {results_df[\"score\"].mean():.4f} (+/- {results_df[\"score\"].std():.4f})')\n",
    "print(f'Mean Sharpe: {results_df[\"sharpe\"].mean():.4f} (+/- {results_df[\"sharpe\"].std():.4f})')\n",
    "print(f'Mean Vol Ratio: {results_df[\"vol_ratio\"].mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Train Final Models on Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last 80% for train, 20% for final validation\n",
    "split_point = int(len(df_clean) * 0.8)\n",
    "final_train = df_clean.iloc[:split_point]\n",
    "final_val = df_clean.iloc[split_point:].copy()\n",
    "\n",
    "print(f'Final training set: {len(final_train)} days')\n",
    "print(f'Final validation set: {len(final_val)} days')\n",
    "\n",
    "# Train final LightGBM\n",
    "print('\\nTraining final LightGBM model...')\n",
    "final_lgb = train_lgb_model(final_train, final_val, feature_list, lgb_params)\n",
    "\n",
    "# Train final XGBoost\n",
    "print('Training final XGBoost model...')\n",
    "final_xgb = train_xgb_model(final_train, final_val, feature_list, xgb_params)\n",
    "\n",
    "print('\\nâœ… Final models trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Final Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on final validation\n",
    "final_val['pred_lgb'] = final_lgb.predict(final_val[feature_list])\n",
    "final_val['pred_xgb'] = final_xgb.predict(xgb.DMatrix(final_val[feature_list]))\n",
    "final_val['pred_ensemble'] = 0.5 * final_val['pred_lgb'] + 0.5 * final_val['pred_xgb']\n",
    "\n",
    "# Calculate allocation\n",
    "final_val['allocation'] = predict_allocation(\n",
    "    final_val['pred_ensemble'],\n",
    "    sensitivity=100,\n",
    "    base_allocation=1.0\n",
    ")\n",
    "\n",
    "# Calculate scores\n",
    "ensemble_score = calculate_competition_score(\n",
    "    final_val['forward_returns'],\n",
    "    final_val['risk_free_rate'],\n",
    "    final_val['allocation']\n",
    ")\n",
    "\n",
    "baseline_score = calculate_competition_score(\n",
    "    final_val['forward_returns'],\n",
    "    final_val['risk_free_rate'],\n",
    "    1.0\n",
    ")\n",
    "\n",
    "print('='*80)\n",
    "print('FINAL VALIDATION RESULTS')\n",
    "print('='*80)\n",
    "\n",
    "print('\\nEnsemble Model:')\n",
    "for key, value in ensemble_score.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f'  {key:25s}: {value:.4f}')\n",
    "\n",
    "print('\\nBaseline (100% invested):')\n",
    "for key, value in baseline_score.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f'  {key:25s}: {value:.4f}')\n",
    "\n",
    "if 'error' not in ensemble_score and 'error' not in baseline_score:\n",
    "    improvement = (ensemble_score['score'] / baseline_score['score'] - 1) * 100\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'IMPROVEMENT OVER BASELINE: {improvement:+.2f}%')\n",
    "    print(f'{\"=\"*80}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Allocation distribution\n",
    "axes[0, 0].hist(final_val['allocation'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 0].axvline(final_val['allocation'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {final_val[\"allocation\"].mean():.2f}')\n",
    "axes[0, 0].set_xlabel('Allocation', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 0].set_title('Allocation Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Allocation over time\n",
    "axes[0, 1].plot(range(len(final_val)), final_val['allocation'], linewidth=1, alpha=0.7)\n",
    "axes[0, 1].axhline(1.0, color='red', linestyle='--', linewidth=1, label='Baseline (1.0)')\n",
    "axes[0, 1].set_xlabel('Time', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Allocation', fontsize=11)\n",
    "axes[0, 1].set_title('Allocation Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cumulative returns\n",
    "strategy_returns = final_val['risk_free_rate'] * (1 - final_val['allocation']) + final_val['allocation'] * final_val['forward_returns']\n",
    "strategy_cumulative = (1 + strategy_returns).cumprod()\n",
    "baseline_cumulative = (1 + final_val['forward_returns']).cumprod()\n",
    "\n",
    "axes[1, 0].plot(range(len(final_val)), strategy_cumulative, linewidth=2, label='Our Strategy', color='green')\n",
    "axes[1, 0].plot(range(len(final_val)), baseline_cumulative, linewidth=2, label='Baseline', color='blue', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Time', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Cumulative Return', fontsize=11)\n",
    "axes[1, 0].set_title('Cumulative Returns Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Prediction vs actual\n",
    "axes[1, 1].scatter(final_val['pred_ensemble'], final_val['forward_returns'], alpha=0.5, s=20)\n",
    "axes[1, 1].plot([final_val['pred_ensemble'].min(), final_val['pred_ensemble'].max()],\n",
    "                [final_val['pred_ensemble'].min(), final_val['pred_ensemble'].max()],\n",
    "                'r--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Predicted Returns', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Actual Returns', fontsize=11)\n",
    "axes[1, 1].set_title('Predicted vs Actual', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "pearson_r, _ = pearsonr(final_val['pred_ensemble'], final_val['forward_returns'])\n",
    "spearman_r, _ = spearmanr(final_val['pred_ensemble'], final_val['forward_returns'])\n",
    "print(f'\\nPrediction Quality:')\n",
    "print(f'  Pearson correlation: {pearson_r:.4f}')\n",
    "print(f'  Spearman correlation: {spearman_r:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "final_lgb.save_model('models/lgb_final.txt')\n",
    "final_xgb.save_model('models/xgb_final.json')\n",
    "\n",
    "# Save feature list and metadata\n",
    "metadata = {\n",
    "    'feature_list': feature_list,\n",
    "    'n_features': len(feature_list),\n",
    "    'cv_mean_score': results_df['score'].mean(),\n",
    "    'cv_std_score': results_df['score'].std(),\n",
    "    'final_val_score': ensemble_score['score'],\n",
    "    'lgb_params': lgb_params,\n",
    "    'xgb_params': xgb_params,\n",
    "}\n",
    "\n",
    "with open('models/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print('âœ… Models saved to models/ directory!')\n",
    "print(f'   - lgb_final.txt')\n",
    "print(f'   - xgb_final.json')\n",
    "print(f'   - metadata.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 11. Kaggle Submission Code\n",
    "\n",
    "This section creates the submission file that works with Kaggle's evaluation API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_code = '''\n",
    "\"\"\"\n",
    "Hull Tactical Market Prediction - Kaggle Submission\n",
    "Enhanced v1 with LightGBM + XGBoost Ensemble\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "# Load models and metadata\n",
    "lgb_model = lgb.Booster(model_file='models/lgb_final.txt')\n",
    "xgb_model = xgb.Booster()\n",
    "xgb_model.load_model('models/xgb_final.json')\n",
    "\n",
    "with open('models/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "feature_list = metadata['feature_list']\n",
    "\n",
    "# Feature engineering function\n",
    "def create_features_for_prediction(test_data, historical_data=None):\n",
    "    \"\"\"\n",
    "    Create features for prediction (must match training features)\n",
    "    \n",
    "    Args:\n",
    "        test_data: Current test data\n",
    "        historical_data: Historical data for lag/rolling features\n",
    "    \"\"\"\n",
    "    # Combine with historical data for lag/rolling calculations\n",
    "    if historical_data is not None:\n",
    "        combined = pd.concat([historical_data, test_data], ignore_index=False)\n",
    "    else:\n",
    "        combined = test_data.copy()\n",
    "    \n",
    "    df = combined.copy()\n",
    "    \n",
    "    # Original features\n",
    "    feature_cols = [c for c in df.columns if c not in ['date_id', 'forward_returns', 'risk_free_rate']]\n",
    "    \n",
    "    # Top features for lag/rolling (from training)\n",
    "    top_features = ['P8', 'P10', 'S5', 'E3', 'M3', 'V13', 'P11', 'E2', 'P12', 'M1', 'S7', 'E5', 'M9', 'V7', 'I5']\n",
    "    \n",
    "    # Lag features\n",
    "    for feat in top_features:\n",
    "        if feat in df.columns:\n",
    "            for lag in [1, 5, 10, 20, 60]:\n",
    "                df[f'{feat}_lag{lag}'] = df[feat].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for feat in top_features:\n",
    "        if feat in df.columns:\n",
    "            for window in [5, 20, 60]:\n",
    "                df[f'{feat}_mean{window}'] = df[feat].rolling(window).mean()\n",
    "                df[f'{feat}_std{window}'] = df[feat].rolling(window).std()\n",
    "    \n",
    "    # Returns-based features (using lagged_forward_returns if available)\n",
    "    if 'lagged_forward_returns' in df.columns:\n",
    "        returns_col = 'lagged_forward_returns'\n",
    "        \n",
    "        df['returns_lag1'] = df[returns_col].shift(1)\n",
    "        df['returns_lag5'] = df[returns_col].shift(5)\n",
    "        \n",
    "        df['momentum_5'] = df[returns_col].rolling(5).sum()\n",
    "        df['momentum_20'] = df[returns_col].rolling(20).sum()\n",
    "        df['momentum_60'] = df[returns_col].rolling(60).sum()\n",
    "        \n",
    "        df['returns_mean_20'] = df[returns_col].rolling(20).mean()\n",
    "        df['returns_std_20'] = df[returns_col].rolling(20).std()\n",
    "        df['returns_mean_60'] = df[returns_col].rolling(60).mean()\n",
    "        df['returns_std_60'] = df[returns_col].rolling(60).std()\n",
    "        \n",
    "        df['volatility_5'] = df[returns_col].rolling(5).std()\n",
    "        df['volatility_20'] = df[returns_col].rolling(20).std()\n",
    "        df['volatility_60'] = df[returns_col].rolling(60).std()\n",
    "        \n",
    "        df['vol_of_vol_20'] = df['volatility_20'].rolling(20).std()\n",
    "        \n",
    "        vol_percentile = df['volatility_20'].rolling(252).rank(pct=True)\n",
    "        df['vol_regime_low'] = (vol_percentile < 0.33).astype(int)\n",
    "        df['vol_regime_high'] = (vol_percentile > 0.67).astype(int)\n",
    "        \n",
    "        df['bull_regime'] = (df['returns_mean_60'] > 0).astype(int)\n",
    "        \n",
    "        df['risk_adj_momentum_20'] = df['momentum_20'] / (df['volatility_20'] + 1e-8)\n",
    "        df['risk_adj_momentum_60'] = df['momentum_60'] / (df['volatility_60'] + 1e-8)\n",
    "    \n",
    "    # Cross-sectional features\n",
    "    feature_categories = {\n",
    "        'M': [c for c in feature_cols if c.startswith('M')],\n",
    "        'E': [c for c in feature_cols if c.startswith('E')],\n",
    "        'I': [c for c in feature_cols if c.startswith('I')],\n",
    "        'P': [c for c in feature_cols if c.startswith('P')],\n",
    "        'V': [c for c in feature_cols if c.startswith('V')],\n",
    "        'S': [c for c in feature_cols if c.startswith('S')],\n",
    "    }\n",
    "    \n",
    "    for cat, feats in feature_categories.items():\n",
    "        if feats:\n",
    "            df[f'{cat}_mean'] = df[feats].mean(axis=1)\n",
    "            df[f'{cat}_std'] = df[feats].std(axis=1)\n",
    "            df[f'{cat}_min'] = df[feats].min(axis=1)\n",
    "            df[f'{cat}_max'] = df[feats].max(axis=1)\n",
    "    \n",
    "    # Feature interactions\n",
    "    if len(top_features) >= 2:\n",
    "        for i in range(min(5, len(top_features))):\n",
    "            for j in range(i+1, min(5, len(top_features))):\n",
    "                feat1, feat2 = top_features[i], top_features[j]\n",
    "                if feat1 in df.columns and feat2 in df.columns:\n",
    "                    df[f'{feat1}_x_{feat2}'] = df[feat1] * df[feat2]\n",
    "    \n",
    "    # Missing value indicator\n",
    "    df['n_missing_orig'] = df[feature_cols].isnull().sum(axis=1)\n",
    "    \n",
    "    # Return only the test data rows\n",
    "    return df.loc[test_data.index]\n",
    "\n",
    "# Allocation function\n",
    "def predict_allocation(predicted_returns, base_allocation=1.0, sensitivity=100):\n",
    "    \"\"\"\n",
    "    Convert predicted returns to allocation (0-2)\n",
    "    \"\"\"\n",
    "    allocation = base_allocation + np.tanh(predicted_returns * sensitivity)\n",
    "    return np.clip(allocation, 0, 2)\n",
    "\n",
    "# Global variables for maintaining history\n",
    "historical_data = None\n",
    "first_call = True\n",
    "\n",
    "def predict(test: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main prediction function called by Kaggle evaluation API\n",
    "    \n",
    "    Args:\n",
    "        test: Test data for current batch\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with date_id and prediction columns\n",
    "    \"\"\"\n",
    "    global historical_data, first_call\n",
    "    \n",
    "    # On first call, load training data for historical features\n",
    "    if first_call:\n",
    "        train_data = pd.read_csv('data/train.csv')\n",
    "        # Keep last 300 days for lag/rolling features\n",
    "        historical_data = train_data.tail(300).copy()\n",
    "        first_call = False\n",
    "    \n",
    "    # Create features\n",
    "    test_with_features = create_features_for_prediction(test, historical_data)\n",
    "    \n",
    "    # Fill missing features with 0 (or use more sophisticated imputation)\n",
    "    for feat in feature_list:\n",
    "        if feat not in test_with_features.columns:\n",
    "            test_with_features[feat] = 0\n",
    "    \n",
    "    test_with_features = test_with_features[feature_list].fillna(0)\n",
    "    \n",
    "    # Predict with ensemble\n",
    "    pred_lgb = lgb_model.predict(test_with_features)\n",
    "    pred_xgb = xgb_model.predict(xgb.DMatrix(test_with_features))\n",
    "    pred_ensemble = 0.5 * pred_lgb + 0.5 * pred_xgb\n",
    "    \n",
    "    # Convert to allocation\n",
    "    allocations = predict_allocation(pred_ensemble, sensitivity=100)\n",
    "    \n",
    "    # Update historical data (keep last 300 days)\n",
    "    historical_data = pd.concat([historical_data, test], ignore_index=False).tail(300)\n",
    "    \n",
    "    # Return predictions\n",
    "    result = pd.DataFrame({\n",
    "        'date_id': test['date_id'],\n",
    "        'prediction': allocations\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Initialize inference server\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Local testing\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))\n",
    "'''\n",
    "\n",
    "# Save submission code\n",
    "with open('submission.py', 'w') as f:\n",
    "    f.write(submission_code)\n",
    "\n",
    "print('âœ… Kaggle submission code saved to submission.py')\n",
    "print('\\nTo submit to Kaggle:')\n",
    "print('1. Copy submission.py to your Kaggle notebook')\n",
    "print('2. Copy models/ directory to your Kaggle notebook')\n",
    "print('3. Run the submission.py script')\n",
    "print('4. Kaggle will evaluate using their API')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 12. Summary & Next Steps\n",
    "\n",
    "### What We Built:\n",
    "- âœ… Advanced feature engineering (100+ features)\n",
    "- âœ… LightGBM + XGBoost ensemble\n",
    "- âœ… Continuous allocation strategy\n",
    "- âœ… Walk-forward cross-validation\n",
    "- âœ… Kaggle submission code\n",
    "\n",
    "### Performance Summary:\n",
    "- Check CV mean score above\n",
    "- Check final validation score above\n",
    "- Compare with baseline (100% invested)\n",
    "\n",
    "### Further Improvements:\n",
    "1. **Hyperparameter tuning**: Use Optuna for more extensive search\n",
    "2. **More models**: Add CatBoost, Neural Networks\n",
    "3. **Feature selection**: Remove redundant features\n",
    "4. **Allocation optimization**: Optimize sensitivity parameter\n",
    "5. **Regime-based models**: Train separate models for different market regimes\n",
    "6. **Stacking**: Use meta-model on top of base models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
